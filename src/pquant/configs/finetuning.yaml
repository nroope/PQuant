pruning_parameters:
    disable_pruning_for_layers:
      []
    pruning_method: pdp
quantization_parameters:
  default_integer_bits: 0.
  default_fractional_bits: 7.
  enable_quantization: true
  hgq_gamma: 0.0003
  hgq_heterogeneous: True
  layer_specific: []
  use_high_granularity_quantization: false
  use_real_tanh: false
  use_symmetric_quantization: false
training_parameters:
   batch_size: 128
   optimizer: sgd
   plot_frequency: 100
   label_smoothing: 0
   model: "resnet18"
   dataset: "cifar10"
   l2_decay:  0.001
   momentum:  0.9
   lr_schedule: "cosine"
   milestones: [30, 80]
   gamma: 0.1
   cosine_tmax: 200
   lr: 0.001
   prune_ratio: 10
   default_integer_bits: 0
   epochs: 2
   fine_tuning_epochs: 2
   pretraining_epochs: 0
   pruning_first: false
   rewind: post-ticket-search
   rounds: 2
   save_weights_epoch: 2
fitcompress_parameters:
  enable_fitcompress : false
  optimize_quantization : true
  quantization_schedule : [7.,4.,3.,2.,1.]
  pruning_schedule : {start : 0, end : -3, steps : 40}
  compression_goal : 0.04
  optimize_pruning : true
  greedy_astar : true
  approximate : true
  f_lambda : 0.5
finetuning_parameters:
    experiment_name: resnet_18_experiment_2
    num_trials: 10
<<<<<<< HEAD
    sampler:
      type: TPESampler
    hyperparameter_search:
      numerical:
        lr: [1e-5, 1e-3, 0.2]
=======
    sampler: TPESampler
    hyperparameter_search:
      numerical:
        learning_rate: [1e-5, 1e-3, 0.2]
>>>>>>> d8cc713 (Add fine-tuning feature v1)
        batch_size: [16, 128, 32]
        default_integer_bits: [0, 8, 1]
      categorical:
        lr_schedule: ["cosine", "multistep"]
