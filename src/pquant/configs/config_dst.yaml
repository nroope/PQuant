pruning_parameters:
  alpha: 5.0e-06
  disable_pruning_for_layers: [] # Disable pruning for these layers, even if enable_pruning is true
  enable_pruning: true
  max_pruning_pct: 0.99
  pruning_method: dst
  threshold_decay: 0.0
  threshold_init: 0.0
  threshold_type: weightwise
quantization_parameters:
  default_weight_keep_negatives: 1.
  default_weight_integer_bits: 0.
  default_weight_fractional_bits: 7.
  default_data_keep_negatives: 0.
  default_data_integer_bits: 0.
  default_data_fractional_bits: 7.
  granularity: "per_tensor"
  quantize_input: true
  quantize_output: false
  enable_quantization: true
  hgq_beta: 1e-5
  hgq_gamma: 0.0003
  hgq_heterogeneous: True
  layer_specific: {}
  use_high_granularity_quantization: false
  use_real_tanh: false
  use_relu_multiplier: false
  use_symmetric_quantization: false
  overflow_mode_parameters: SAT
  overflow_mode_data: SAT
  round_mode: RND
fitcompress_parameters:
  enable_fitcompress : false
  optimize_quantization : true
  quantization_schedule : [7.,4.,3.,2.,1.]
  pruning_schedule : {start : 0, end : -3, steps : 40}
  compression_goal : 0.10
  optimize_pruning : false
  greedy_astar : true
  approximate : true
  f_lambda : 1
training_parameters:
  epochs: 160
  fine_tuning_epochs: 0
  pretraining_epochs: 0
  pruning_first: true
  rewind: never
  rounds: 1
  save_weights_epoch: -1
hpo_parameters:
    experiment_name: experiment_name
    model_name: jet_tagger
    num_trials: 1
    sampler:
      type: RandomSampler
    hyperparameter_search:
      numerical: {}
      categorical: {}
